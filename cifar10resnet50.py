# -*- coding: utf-8 -*-
"""Cifar10Resnet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qL0CMLSIEhYwlfglFqzIA0RyqKBlN8AQ
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
import tensorflow.keras as keras
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
import tensorflow as tf
from keras.utils import np_utils
from keras.models import load_model
from keras.datasets import cifar10
from keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2



"""**Importing Libraries**"""

resnetModel = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

"""**Importing resnet 50 model**"""

(trainInput , trainLabel) , (testInput, testLabel) = cifar10.load_data()

trainInput = trainInput / 255.0
testInput = testInput / 255.0

"""**Getting cifar 10 data**"""

trainLabel = np_utils.to_categorical(trainLabel, 10)
testLabel = np_utils.to_categorical(testLabel, 10)

"""**converting label to one hot encoding**"""

model = models.Sequential()

#encoder
model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2), padding='same') )
# model.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same') )
# model.add(layers.MaxPooling2D((2, 2), padding='same') )
# model.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same') )
# model.add(layers.MaxPooling2D((2, 2), padding='same') )

#decoder
# model.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same') )
# model.add(layers.UpSampling2D((2, 2)) )
# model.add (layers.Conv2D(8, (3, 3), activation='relu', padding='same') )
# model.add(layers.UpSampling2D((2, 2)) )
model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(layers.UpSampling2D((2, 2)) )

model.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same') )

model.compile(optimizer='adadelta', loss='binary_crossentropy')

"""**Feature extraction model**"""

model.fit(trainInput, trainInput,
                epochs=50,
                batch_size=128,
                shuffle=True,
                validation_data=(testInput, testInput))

"""**Training autoencoder**"""

model = models.Sequential()

#upsampling to increase image....
model.add(layers.UpSampling2D((2,2)))
model.add(layers.UpSampling2D((2,2)))
model.add(layers.UpSampling2D((2,2)))

model.add(resnetModel)

model.add(layers.Flatten())
model.add(layers.BatchNormalization())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.BatchNormalization())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.BatchNormalization())
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss='categorical_crossentropy', metrics=['acc'])

"""**Preparing model with the help of resnet 50 pretrained model with imagenet**"""

from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(horizontal_flip = True)

"""**data augmentation**"""

import time
start = time.time()
history = model.fit_generator(datagen.flow(trainInput, trainLabel,batch_size = 20), epochs=5, validation_data=(testInput, testLabel), steps_per_epoch = len(trainInput) // 20)
end = time.time()

print("Total Training time: ",(end-start))

"""**Training time**"""

import keras
from matplotlib import pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
plt.savefig("performance_cifar10.png")

"""**Analyzing performance**

**training model**
"""

score = model.evaluate(testInput, testLabel)
print('The model achieved a accuracy of %.2f%%.' % (score[1]*100))

"""**evaluating model performance**"""
